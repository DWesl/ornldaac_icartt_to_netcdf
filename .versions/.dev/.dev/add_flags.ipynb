{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:31.053283Z",
     "start_time": "2019-08-19T19:54:24.308037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTAMERICA-mrg05-b200_merge_20170201_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170204_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170206_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170212_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170216_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170223_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170225_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170301_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170304_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20170307_R1.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171003_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171004_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171005_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171008_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171010_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171011_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171014_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171016_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171018_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171020_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171021_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171022_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171024_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171026_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171027_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171030_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171102_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171103_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171105_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171106_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171109_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20171110_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180412_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180413_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180414_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180416_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180417_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180418_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180419_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180420_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180422_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180423_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180425_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180426_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180427_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180429_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180430_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180501_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180502_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180504_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180508_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180509_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180511_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180512_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180515_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180516_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180518_R0.nc\r\n",
      "ACTAMERICA-mrg05-b200_merge_20180520_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20160718_R3.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20170309_R1.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171003_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171004_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171005_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171008_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171010_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171011_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171014_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171016_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171018_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171020_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171021_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171022_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171024_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171026_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171027_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171030_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171102_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171103_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171105_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171106_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171109_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20171110_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180425_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180426_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180427_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180429_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180430_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180501_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180502_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180504_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180508_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180509_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180511_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180512_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180515_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180516_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180518_R0.nc\r\n",
      "ACTAMERICA-mrg05-c130_merge_20180520_R0.nc\r\n",
      "add_flags.ipynb\r\n",
      "bad\r\n",
      "flags_v2\r\n",
      "matches.json\r\n",
      "mrg05\r\n",
      "scratch.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import netCDF4 as nc4\n",
    "\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:34.201301Z",
     "start_time": "2019-08-19T19:54:31.879186Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 mrg05/*.nc | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:35.599924Z",
     "start_time": "2019-08-19T19:54:35.586596Z"
    }
   },
   "outputs": [],
   "source": [
    "ncfiles = glob.glob(\"mrg05/*.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:38.503732Z",
     "start_time": "2019-08-19T19:54:36.791502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 flags_v2/*.ict | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:39.374494Z",
     "start_time": "2019-08-19T19:54:39.331960Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B200_20180412',\n",
       " 'B200_20180414',\n",
       " 'C130_20170204',\n",
       " 'C130_20180427',\n",
       " 'B200_20180518',\n",
       " 'B200_20160812',\n",
       " 'B200_20160816',\n",
       " 'B200_20170217',\n",
       " 'C130_20171102',\n",
       " 'B200_20171004',\n",
       " 'B200_20180504',\n",
       " 'C130_20170302',\n",
       " 'B200_20180429',\n",
       " 'B200_20180515',\n",
       " 'B200_20160828',\n",
       " 'C130_20160819',\n",
       " 'C130_20160810',\n",
       " 'B200_20180419',\n",
       " 'B200_20160718',\n",
       " 'B200_20171024',\n",
       " 'B200_20171010',\n",
       " 'C130_20170215',\n",
       " 'C130_20170307',\n",
       " 'B200_20170308',\n",
       " 'C130_20160820',\n",
       " 'C130_20160719',\n",
       " 'B200_20180520',\n",
       " 'C130_20171021',\n",
       " 'B200_20171008',\n",
       " 'B200_20170227',\n",
       " 'B200_20160727',\n",
       " 'B200_20160722',\n",
       " 'B200_20160820',\n",
       " 'B200_20160808',\n",
       " 'B200_20170204',\n",
       " 'C130_20180512',\n",
       " 'B200_20171022',\n",
       " 'B200_20180420',\n",
       " 'C130_20180518',\n",
       " 'C130_20171106',\n",
       " 'C130_20180509',\n",
       " 'C130_20170226',\n",
       " 'B200_20171018',\n",
       " 'C130_20160725',\n",
       " 'C130_20160804',\n",
       " 'C130_20160721',\n",
       " 'C130_20170301',\n",
       " 'C130_20171008',\n",
       " 'B200_20180516',\n",
       " 'B200_20171105',\n",
       " 'B200_20160814',\n",
       " 'B200_20170212',\n",
       " 'B200_20180430',\n",
       " 'B200_20170215',\n",
       " 'B200_20171030',\n",
       " 'C130_20171105',\n",
       " 'C130_20170310',\n",
       " 'C130_20160824',\n",
       " 'B200_20170216',\n",
       " 'B200_20170208',\n",
       " 'C130_20171005',\n",
       " 'B200_20170225',\n",
       " 'C130_20180502',\n",
       " 'B200_20170213',\n",
       " 'C130_20180520',\n",
       " 'C130_20170303',\n",
       " 'C130_20170213',\n",
       " 'B200_20180425',\n",
       " 'C130_20171024',\n",
       " 'B200_20171106',\n",
       " 'C130_20160809',\n",
       " 'C130_20170201',\n",
       " 'B200_20170210',\n",
       " 'B200_20160813',\n",
       " 'B200_20170206',\n",
       " 'C130_20171004',\n",
       " 'C130_20171110',\n",
       " 'B200_20160721',\n",
       " 'B200_20180512',\n",
       " 'B200_20160819',\n",
       " 'B200_20160824',\n",
       " 'C130_20180501',\n",
       " 'C130_20171003',\n",
       " 'C130_20160822',\n",
       " 'C130_20160803',\n",
       " 'B200_20160823',\n",
       " 'B200_20180422',\n",
       " 'C130_20160718',\n",
       " 'B200_20171003',\n",
       " 'C130_20170304',\n",
       " 'B200_20180413',\n",
       " 'B200_20170223',\n",
       " 'C130_20180429',\n",
       " 'C130_20180508',\n",
       " 'C130_20170225',\n",
       " 'B200_20170307',\n",
       " 'C130_20160813',\n",
       " 'B200_20170220',\n",
       " 'B200_20180511',\n",
       " 'C130_20160814',\n",
       " 'C130_20180426',\n",
       " 'B200_20160805',\n",
       " 'B200_20171016',\n",
       " 'C130_20160816',\n",
       " 'C130_20160808',\n",
       " 'C130_20171103',\n",
       " 'B200_20160822',\n",
       " 'C130_20160727',\n",
       " 'B200_20180426',\n",
       " 'B200_20160821',\n",
       " 'C130_20170227',\n",
       " 'C130_20171010',\n",
       " 'C130_20171030',\n",
       " 'B200_20170219',\n",
       " 'B200_20160725',\n",
       " 'B200_20170301',\n",
       " 'C130_20160726',\n",
       " 'C130_20171027',\n",
       " 'B200_20170209',\n",
       " 'B200_20171110',\n",
       " 'B200_20171027',\n",
       " 'C130_20160821',\n",
       " 'B200_20160804',\n",
       " 'B200_20180423',\n",
       " 'B200_20160809',\n",
       " 'C130_20170308',\n",
       " 'C130_20171016',\n",
       " 'B200_20171026',\n",
       " 'B200_20180502',\n",
       " 'C130_20180516',\n",
       " 'B200_20171021',\n",
       " 'B200_20171014',\n",
       " 'B200_20171005',\n",
       " 'B200_20170304',\n",
       " 'B200_20171109',\n",
       " 'C130_20171011',\n",
       " 'C130_20160827',\n",
       " 'B200_20171011',\n",
       " 'C130_20170206',\n",
       " 'B200_20160726',\n",
       " 'C130_20171026',\n",
       " 'C130_20171020',\n",
       " 'C130_20180425',\n",
       " 'C130_20160812',\n",
       " 'B200_20170201',\n",
       " 'C130_20180511',\n",
       " 'B200_20171103',\n",
       " 'C130_20180504',\n",
       " 'B200_20180418',\n",
       " 'C130_20171022',\n",
       " 'C130_20180515',\n",
       " 'C130_20170216',\n",
       " 'B200_20160827',\n",
       " 'C130_20170212',\n",
       " 'B200_20180416',\n",
       " 'C130_20171109',\n",
       " 'C130_20160805',\n",
       " 'B200_20180427',\n",
       " 'C130_20170309',\n",
       " 'C130_20180430',\n",
       " 'B200_20171102',\n",
       " 'C130_20171014',\n",
       " 'B200_20180417',\n",
       " 'B200_20180509',\n",
       " 'C130_20171018',\n",
       " 'B200_20160803',\n",
       " 'B200_20180501',\n",
       " 'B200_20171020',\n",
       " 'B200_20180508',\n",
       " 'C130_20160722',\n",
       " 'C130_20170223']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(\"flags_v2/*.ict\")\n",
    "flights = [os.path.basename(f)[25:38] for f in files]\n",
    "uflights = list(set(flights))\n",
    "uflightsd = {}\n",
    "\n",
    "for flt0 in uflights:\n",
    "    t = []\n",
    "    for flt1 in files:\n",
    "        if flt0 in flt1:\n",
    "            t.append(flt1)\n",
    "    t = sorted(t)\n",
    "    if len(t)>1:\n",
    "        if ((\"_L1_\" in t[-2]) & (\"_L2_\" in t[-1])):\n",
    "            uflightsd[flt0] = t\n",
    "        else:\n",
    "            uflightsd[flt0] = t[-1]\n",
    "    else:\n",
    "        uflightsd[flt0] = t[0]\n",
    "\n",
    "uflights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:41.373675Z",
     "start_time": "2019-08-19T19:54:40.932339Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uflightsd2 = {}\n",
    "for k, v in uflightsd.items():\n",
    "    file_ = None\n",
    "    for f in ncfiles:\n",
    "        fcomp = os.path.basename(f).split(\"_\")\n",
    "        aircraft = fcomp[0].split(\"-\")[-1].upper()\n",
    "        date = fcomp[-2]  \n",
    "        if ((aircraft in k) & (date in k)):\n",
    "            file_ = f\n",
    "    uflightsd2[k] = (file_, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:42.300459Z",
     "start_time": "2019-08-19T19:54:41.928080Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"matches.json\", \"w\") as f:\n",
    "    f.write(json.dumps(uflightsd2, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in flights:\n",
    "    if f not in uflightsd2:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(flights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:58:04.702666Z",
     "start_time": "2019-08-19T19:54:49.875939Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_multileg(icartt):\n",
    "    dfs = {}\n",
    "    for ict in icartt:\n",
    "        with open(ict, \"r\") as f:\n",
    "            headerrow = int(f.readlines()[0].split(\",\")[0])-1\n",
    "        dfs[ict] = pd.read_csv(ict, header=headerrow, delimiter=\",\")\n",
    "    df1, df2 = dfs.values()\n",
    "    df1 = df1.append(df2)\n",
    "    df1.columns = [c.strip() for c in list(df1.columns)]\n",
    "    return df1\n",
    "    \n",
    "def read_singleleg(icartt):\n",
    "    with open(icartt, \"r\") as f:\n",
    "        headerrow = int(f.readlines()[0].split(\",\")[0])-1\n",
    "    df = pd.read_csv(icartt, header=headerrow, delimiter=\",\")\n",
    "    df.columns = [c.strip() for c in list(df.columns)]\n",
    "    return df\n",
    "\n",
    "def read_flight(flight_tuple):\n",
    "    icartt = flight_tuple[1]\n",
    "    flight_tuple2 = [flight_tuple[0], None]\n",
    "    if type(icartt) is list:\n",
    "        flight_tuple2[1] = read_multileg(icartt)\n",
    "    else:\n",
    "        flight_tuple2[1] = read_singleleg(icartt)\n",
    "    return flight_tuple2\n",
    "\n",
    "\n",
    "files = glob.glob(\"flags_v2/*.ict\")\n",
    "flights = [os.path.basename(f)[25:38] for f in files]\n",
    "uflights = list(set(flights))\n",
    "uflightsd = {}\n",
    "\n",
    "for flt0 in uflights:\n",
    "    t = []\n",
    "    for flt1 in files:\n",
    "        if flt0 in flt1:\n",
    "            t.append(flt1)\n",
    "    t = sorted(t)\n",
    "    if len(t)>1:\n",
    "        if ((\"_L1_\" in t[-2]) & (\"_L2_\" in t[-1])):\n",
    "            uflightsd[flt0] = t\n",
    "        else:\n",
    "            uflightsd[flt0] = t[-1]\n",
    "    else:\n",
    "        uflightsd[flt0] = t[0]\n",
    "\n",
    "print(uflightsd)\n",
    "\n",
    "uflightsd2 = {}\n",
    "for k, v in uflightsd.items():\n",
    "    file_ = None\n",
    "    for f in ncfiles:\n",
    "        fcomp = os.path.basename(f).split(\"_\")\n",
    "        aircraft = fcomp[0].split(\"-\")[-1].upper()\n",
    "        date = fcomp[-2]  \n",
    "        if ((aircraft in k) & (date in k)):\n",
    "            file_ = f\n",
    "    uflightsd2[k] = (file_, v)\n",
    "\n",
    "print(uflightsd2)\n",
    "    \n",
    "uflightsd3 = {}\n",
    "for k, v in uflightsd2.items():\n",
    "    try:\n",
    "        uflightsd3[k] = read_flight(v)\n",
    "    except:\n",
    "        uflightsd3[k] = None\n",
    "        print(k)\n",
    "        pass\n",
    "    \n",
    "print(uflightsd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "UNEQUAL = []\n",
    "FAILS =[]\n",
    "for flight, data in uflightsd3.items():\n",
    "    fname = os.path.basename(data[0])\n",
    "    with nc4.Dataset(data[0]) as src, nc4.Dataset(fname, \"w\") as dst:\n",
    "        \n",
    "        if data[1].index.size == src.dimensions[\"time\"].size:\n",
    "        \n",
    "            # copy global attributes all at once via dictionary\n",
    "            dst.setncatts(src.__dict__)\n",
    "\n",
    "            # copy dimensions\n",
    "            for name, dimension in src.dimensions.items():\n",
    "                dst.createDimension(\n",
    "                    name, (len(dimension) if not dimension.isunlimited() else None))\n",
    "\n",
    "            # copy all file data except for the excluded\n",
    "            for name, variable in src.variables.items():\n",
    "                try:\n",
    "                    x = dst.createVariable(\n",
    "                        name, \n",
    "                        variable.datatype, \n",
    "                        variable.dimensions, \n",
    "                        fill_value=variable._FillValue,\n",
    "                        zlib=True)\n",
    "                    del src[name].__dict__[\"_FillValue\"]\n",
    "                    dst[name].setncatts(src[name].__dict__)\n",
    "                except:\n",
    "                    x = dst.createVariable(name, variable.datatype, variable.dimensions, zlib=True)\n",
    "                    dst[name].setncatts(src[name].__dict__)\n",
    "                dst[name][:] = src[name][:]\n",
    "                # copy variable attributes all at once via dictionary\n",
    "                \n",
    "\n",
    "    \n",
    "            try:\n",
    "\n",
    "                FlightPatternFlag = dst.createVariable('FlightPatternFlag', 'int32', ('time'), fill_value=-999999, zlib=True)\n",
    "                FlightPatternFlag.long_name = \"Flight pattern description\"\n",
    "                FlightPatternFlag.coordinates = \"time LATITUDE LONGITUDE GPS_ALT\"\n",
    "                FlightPatternFlag.flag_meanings = \"frontal pre-frontal_fair post-frontal_fair fair other\"\n",
    "                FlightPatternFlag.valid_range = (0, 4)\n",
    "                FlightPatternFlag.flag_values = (0, 1, 2, 3, 4)\n",
    "                FlightPatternFlag[:] = data[1][\"Flight_flag\"].fillna(-999999).astype(int).to_numpy()\n",
    "\n",
    "                if \"ManeuverFlag\" not in list(dst.variables.keys()):\n",
    "                    ManeuverFlag = dst.createVariable('ManeuverFlag', 'int32', ('time'), fill_value=-999999,zlib=True)\n",
    "                    ManeuverFlag.long_name = \"data flag for sampling based on aircraft maneuver\"\n",
    "                    ManeuverFlag.coordinates = \"time LATITUDE LONGITUDE GPS_ALT\"\n",
    "                    ManeuverFlag.flag_meanings = \"on_ground take_off in-line_ascent in-line_descent spiral_up spiral_down constant_alt_legs landing\"\n",
    "                    ManeuverFlag.valid_range = (0, 7)\n",
    "                    ManeuverFlag.flag_values = (0, 1, 2, 3, 4, 5, 6, 7)\n",
    "                    ManeuverFlag[:] = data[1][\"Maneuver_flag\"].fillna(-999999).astype(int).to_numpy()\n",
    "\n",
    "                ManeuverFlagQC = dst.createVariable('ManeuverFlagQC', 'int32', ('time'), fill_value=-999999,zlib=True)\n",
    "                ManeuverFlagQC.long_name = \"Low or average confidence in ManeuverFlag\"\n",
    "                ManeuverFlagQC.coordinates = \"time LATITUDE LONGITUDE GPS_ALT\"\n",
    "                ManeuverFlagQC.flag_meanings = \"low_confidence average_confidence\"\n",
    "                ManeuverFlagQC.valid_range = (0, 1)\n",
    "                ManeuverFlagQC.flag_values = (0, 1)\n",
    "                ManeuverFlagQC[:] = data[1][\"Maneuver_flagQC\"].fillna(-999999).astype(int).to_numpy()\n",
    "\n",
    "                BoundaryLayerFreeTroposphereFlag = dst.createVariable('BoundaryLayerFreeTroposphereFlag', 'int32', ('time'), fill_value=-999999, zlib=True)\n",
    "                BoundaryLayerFreeTroposphereFlag.long_name = \"Flag indicating whether aircraft is grounded, in boundary layer, or in free troposphere\"\n",
    "                BoundaryLayerFreeTroposphereFlag.coordinates = \"time LATITUDE LONGITUDE GPS_ALT\"\n",
    "                BoundaryLayerFreeTroposphereFlag.flag_meanings = \"on_ground boundary_layer free_troposphere\"\n",
    "                BoundaryLayerFreeTroposphereFlag.valid_range = (0, 2)\n",
    "                BoundaryLayerFreeTroposphereFlag.flag_values = (0, 1, 2)\n",
    "                BoundaryLayerFreeTroposphereFlag[:] = data[1][\"BL_FT_flag\"].fillna(-999999).astype(int).to_numpy()\n",
    "\n",
    "                WarmColdAirFlag = dst.createVariable('WarmColdAirFlag', 'int32', ('time'), fill_value=-999999, zlib=True)\n",
    "                WarmColdAirFlag.long_name = \"Flag indicating whether aircraft is in warm or cold air, or fair conditions.\"\n",
    "                WarmColdAirFlag.coordinates = \"time LATITUDE LONGITUDE GPS_ALT\"\n",
    "                WarmColdAirFlag.flag_meanings = \"fair cold warm\"\n",
    "                WarmColdAirFlag.valid_range = (0, 2)\n",
    "                WarmColdAirFlag.flag_values = (0, 1, 2)\n",
    "                WarmColdAirFlag[:] = data[1][\"Air_flag\"].fillna(-999999).astype(int).to_numpy()\n",
    "\n",
    "            except Exception as e:\n",
    "                FAILS.append((flight,e))\n",
    "                print(flight)\n",
    "                print(e)\n",
    "        else:\n",
    "            UNEQUAL.append(flight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Merged data file for ACT-AMERICA 2018, Flight 19 (20180508), on the b200 platform. Data is merged to PFP file timeline.',\n",
       " 'mission_name': 'NASA ACT-AMERICA 2018 Summer Campaign',\n",
       " 'flight_date': '2018-05-08',\n",
       " 'platform': 'NASA B200 Aircraft',\n",
       " 'location': 'Latitude, Longitude, and Altitude included in data records',\n",
       " 'associated_data': 'N/A',\n",
       " 'instrument_info': 'N/A',\n",
       " 'data_info': 'Please see PI data file for explanation of ULOD and LLOD.',\n",
       " 'uncertainty': 'See individual files for uncertainty information.',\n",
       " 'ulod_flag': '-777777',\n",
       " 'ulod_value': 'N/A',\n",
       " 'llod_flag': '-888888',\n",
       " 'llod_value': 'N/A',\n",
       " 'project_info': 'ACT-America',\n",
       " 'comments': 'This merge was created using data in the NASA ACT-AMERICA archive as of 10/12/2018. In most cases, variable names have been kept identical to those submitted in the raw data files. However, in some cases, names have been changed  (e.g., to eliminate duplication)  Units have been standardized throughout the merge. Contact michael.shook@nasa.gov with any questions as to which data was included.',\n",
       " 'ancillary_file_source': 'ACTAMERICA-mrgPFP-b200_merge_20180508_R0.ict ',\n",
       " 'revision': 'R0; R0: Includes data submitted as of 10/12/2018. See the readme file for a more detailed changelog.; ',\n",
       " 'featureType': 'trajectory',\n",
       " 'id': '10.3334/ORNLDAAC/1593',\n",
       " 'naming_authority': 'ORNL DAAC',\n",
       " 'institution': 'Oak Ridge National Laboratory (ORNL) Distributed Active Archive Center (DAAC)',\n",
       " 'creator_url': 'https://daac.ornl.gov',\n",
       " 'creator_email': 'uso@daac.ornl.gov',\n",
       " 'citation': \"Davis, K.J., M.D. Obland, B. Lin, T. Lauvaux, C. O'dell, B. Meadows, E.V. Browell, J.H. Crawford, J.P. Digangi, C. Sweeney, M.J. McGill, J. Dobler, J.D. Barrick, and A.R. Nehrir. 2018. ACT-America: L3 Merged In Situ Atmospheric Trace Gases and Flask Data, Eastern USA. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1593\",\n",
       " 'summary': \"This dataset provides three merged data products acquired during flights over the central and eastern United States as part of the Atmospheric Carbon and Transport - America (ACT-America) project. Two aircraft platforms, the NASA Langley Beechcraft B200 King Air and the NASA Goddard Space Flight Center's C-130H Hercules, were used to collect high-quality in situ measurements across a variety of continental surfaces and atmospheric conditions. The merged data products are composed of continuous in situ measurements of atmospheric carbon dioxide (CO2), methane (CH4), carbon monoxide (CO), ozone (O3), and ethane (C2H6, B200 aircraft only) that were averaged to uniform intervals and merged with trace gas concentrations from discrete flask samples collected with the Programmable Flask Package (PFP) and the aircraft navigation and meteorological variables. These merged data products provide integrated measurements at intervals useful to the modeling community for studying the transport and fluxes of atmospheric carbon dioxide and methane across North America.\",\n",
       " 'keywords': ['atmosphere > atmospheric chemistry > carbon and hydrocarbon compounds > carbon dioxide',\n",
       "  'atmosphere > atmospheric chemistry > carbon and hydrocarbon compounds > carbon monoxide',\n",
       "  'atmosphere > atmospheric chemistry > halocarbons and halogens',\n",
       "  'atmosphere > atmospheric chemistry > trace gases/trace species',\n",
       "  'atmosphere > atmospheric chemistry > carbon and hydrocarbon compounds',\n",
       "  'atmosphere > atmospheric chemistry > carbon and hydrocarbon compounds > methane',\n",
       "  'atmosphere > atmospheric chemistry > oxygen compounds > ozone'],\n",
       " 'keywords_vocabulary': 'GCMD Science Keywords',\n",
       " 'stipulations_on_use': 'Data available with no restrictions.',\n",
       " 'Conventions': 'CF-1.6'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import netCDF4 as nc4\n",
    "\n",
    "\n",
    "def get_variable(variable):\n",
    "    \"\"\"\n",
    "    Returns a dictionary describing the variables in a netCDF file.\n",
    "    \"\"\"\n",
    "    \n",
    "    structure = {}\n",
    "    \n",
    "    # Add the dimensions and attributes of the variables in a loop.\n",
    "\n",
    "        \n",
    "    return structure\n",
    "\n",
    "\n",
    "def get_netcdf_dataset_structure(dataset):\n",
    "    \"\"\"\n",
    "    Makes a JSON (Panoply-like) representation of netCDF structure. \n",
    "    Groups are also dataset constructs in netCDF4 python.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # netCDF structure as a dictionary. \n",
    "    structure = {\n",
    "        'groups': {},\n",
    "        'variables': {}, \n",
    "        'attributes': dataset.__dict__,\n",
    "    }\n",
    "    \n",
    "    # VARIABLES: Add this dataset's variables to structure dictionary.\n",
    "    for name, variable in dataset.variables.items():\n",
    "        \n",
    "        structure['variables'][name] = {\n",
    "            'dimensions': variable.dimensions,\n",
    "            'attributes': variable.__dict__,\n",
    "        }\n",
    "\n",
    "    # GROUPS: are also datasets. Loop and repeatedly call this function.\n",
    "    for name, group in dataset.groups.items():\n",
    "        structure['groups'][name] = get_netcdf_dataset_structure(group)\n",
    "    \n",
    "    return structure\n",
    "\n",
    "\n",
    "def get_netcdf_file_structure(netcdf_file_path: str):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read netCDF and get dictionary structure.\n",
    "    with nc4.Dataset(netcdf_file_path) as ds:\n",
    "        structure = get_netcdf_dataset_structure(ds)\n",
    "    \n",
    "    # Add dimensions to the dictionary structure.\n",
    "    for name, dimension in dataset.dimensions.items():\n",
    "\n",
    "        structure['dimensions'][name] = {\n",
    "            'UNLIMITED': dimension.isunlimited(),\n",
    "            'size': dimension.size()\n",
    "        }\n",
    "    \n",
    "    return structure\n",
    "\n",
    "\n",
    "def construct_PATH_OUTPUT(input_file_path: str, PATH_OUTPUT: str, output_file_extension: str):\n",
    "    '''\n",
    "    Simple path constructor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Get output file basename.\n",
    "    output_file_basename = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "    \n",
    "    # Get output file name.\n",
    "    output_file_name = \"{}.{}\".format(output_file_basename, output_file_extension)\n",
    "\n",
    "    # Use os module to join path.\n",
    "    output_file_path = os.path.join(PATH_OUTPUT, output_file_name)\n",
    "\n",
    "    return output_file_path\n",
    "\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    '''\n",
    "    A well known hack for serializing numpy types to JSON.\n",
    "    '''\n",
    "    numpy_ints = (  # numpy integer data types.\n",
    "        np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, \n",
    "        np.int64, np.uint8, np.uint16, np.uint32, np.uint64)\n",
    "    numpy_flts = (  # numpy float data types.\n",
    "        np.float_, np.float16, np.float32, np.float64)\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, self.numpy_ints):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, self.numpy_flts):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def _write_numpy_yaml(dictionary: dict, output: str):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    # Dump to YAML string.\n",
    "    dict_yaml = yaml.dump(dictionary)  ##, indent=2)) #, cls=NumpyJsonEncoder))\n",
    "\n",
    "    # Write output YAML.\n",
    "    with open(output, \"w\") as f:\n",
    "        f.write(dict_yaml)\n",
    "\n",
    "    \n",
    "def _write_numpy_json(dictionary: dict, output: str):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Dump to JSON string.\n",
    "    dict_json = json.dumps(dictionary, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    # And write output JSON.\n",
    "    with open(output, \"w\") as f:\n",
    "        f.write(dict_json)\n",
    "\n",
    "\n",
    "def get_netcdf_file_structure(\n",
    "    input_path_netcdf: str, \n",
    "    PATH_OUTPUT_json:str=None, \n",
    "    PATH_OUTPUT_yaml:str=None):\n",
    "    '''\n",
    "    Get the complete hierarchical structure of a netCDF file as a Python dict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Open the nteCDF file.\n",
    "    netcdf_dataset = nc4.Dataset(input_path_netcdf)\n",
    "   \n",
    "    # Get the structure of the dataset root.\n",
    "    root_structure = get_netcdf_dataset_structure(netcdf_dataset)\n",
    "    \n",
    "    # Close the open netCDF dataset.\n",
    "    netcdf_dataset.close()\n",
    "       \n",
    "    # If JSON path was given as arg, write to disk.\n",
    "    if PATH_OUTPUT_json is not None:\n",
    "        _write_numpy_json(root_structure, PATH_OUTPUT_json)\n",
    "\n",
    "    # If YAML path was given as arg, write to disk.\n",
    "    if PATH_OUTPUT_yaml is not None:\n",
    "        _write_numpy_yaml(root_structure, PATH_OUTPUT_yaml)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "netcdf = glob.glob(\"/data/actamerica/ACTAMERICA_Merge/data/nc/*.nc*\")\n",
    "\n",
    "test_in_nc = '/data/actamerica/ACTAMERICA_Merge/data/nc/ACTAMERICA-mrgPFP-b200_merge_20180508_R0.nc'\n",
    "\n",
    "test_out_dir = '/home/jnd/Documents/missions/evs_mission_support/netcdf_services/resources/netcdf_headers_parsed/'\n",
    "\n",
    "# Get output paths.\n",
    "output_file_json = construct_PATH_OUTPUT(test_in_nc, test_out_dir, \"json\")\n",
    "output_file_yaml = construct_PATH_OUTPUT(test_in_nc, test_out_dir, \"yaml\")\n",
    "\n",
    "# Write the structure file.\n",
    "get_netcdf_file_structure(\n",
    "    input_path_netcdf=test_in_nc, \n",
    "    PATH_OUTPUT_json=output_file_json, \n",
    "    PATH_OUTPUT_yaml=output_file_yaml\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actamerica_config.json\ticartt_headers_parsed  netcdf_headers_parsed\r\n"
     ]
    }
   ],
   "source": [
    "!ls \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi1 = [i for i in icartts if \"_L1\" in i]\n",
    "multi2 = [i for i in icartts if \"_L2\" in i]\n",
    "print(multi1)\n",
    "print(multi2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
